{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b37faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from research.mtm.models.mtm_model import MTM, make_plots_with_masks\n",
    "from research.mtm.tokenizers.base import Tokenizer, TokenizerManager\n",
    "from research.mtm.train import main, create_eval_logs_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05956960",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"research/mtm\"):\n",
    "    cfg = compose(config_name=\"config.yaml\", overrides=[\"+exp_mtm=sinusoid_cont\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0073764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosmos/VScode Projects/AIRIproject/mtm/sinusoid_exp\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"sinusoid_exp\", exist_ok=True)\n",
    "os.chdir(\"sinusoid_exp\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c631c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:research.mtm.distributed_utils:Loading distributed job info from submitit...\n",
      "WARNING:research.mtm.distributed_utils:Unable to load from submitit JobEnvironment: No module named 'submitit'\n",
      "INFO:research.mtm.distributed_utils:Loading distributed job info from environment variables...\n",
      "WARNING:research.mtm.distributed_utils:Unable to load from environment variables: 'RANK'\n",
      "INFO:root:Working directory: /home/cosmos/VScode Projects/AIRIproject/mtm/sinusoid_exp\n",
      "INFO:root:Train set size = 10000\n",
      "INFO:root:Validation set size = 512\n",
      "INFO:root:Tokenizers: {'states': ContinuousTokenizer()}\n",
      "INFO:root:\u001b[33m[stopwatch: data loader] Starting!\u001b[0m\n",
      "INFO:root:\u001b[33m[stopwatch: data loader] Completed in \u001b[1m0.001249551773071289 seconds\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig(seed=0,\n",
      "          batch_size=1024,\n",
      "          n_workers=10,\n",
      "          log_every=100,\n",
      "          print_every=1000,\n",
      "          eval_every=500,\n",
      "          save_every=10000,\n",
      "          device='cpu',\n",
      "          mask_ratios=[0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
      "          mask_patterns=['AUTO_MASK'],\n",
      "          warmup_steps=100,\n",
      "          num_train_steps=1010,\n",
      "          learning_rate=0.0001,\n",
      "          weight_decay=0.005,\n",
      "          traj_length=16,\n",
      "          mode_weights=[0.2, 0.1, 0.7],\n",
      "          tsp_ratio=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data shapes: {'states': torch.Size([1, 1])}\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskvayzer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1783ab5b8be94f43a190b6d21ab18655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668995899999574, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>logs/1690463975718506719-0/wandb/run-20230727_161950-1690463975718506719-0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skvayzer/experiments/runs/1690463975718506719-0' target=\"_blank\">1690463975718506719-0</a></strong> to <a href='https://wandb.ai/skvayzer/experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skvayzer/experiments' target=\"_blank\">https://wandb.ai/skvayzer/experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skvayzer/experiments/runs/1690463975718506719-0' target=\"_blank\">https://wandb.ai/skvayzer/experiments/runs/1690463975718506719-0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:starting from step=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes torch.Size([1024, 16, 1, 512])\n",
      "shapes torch.Size([1, 1, 1, 512])\n",
      "shapes torch.Size([1, 16, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Step: 0, Train Loss: 0.9640020132064819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes torch.Size([1024, 16, 1, 512])\n",
      "shapes torch.Size([1, 1, 1, 512])\n",
      "shapes torch.Size([1, 16, 1, 512])\n",
      "shapes torch.Size([1024, 16, 1, 512])\n",
      "shapes torch.Size([1, 1, 1, 512])\n",
      "shapes torch.Size([1, 16, 1, 512])\n",
      "shapes torch.Size([1024, 16, 1, 512])\n",
      "shapes torch.Size([1, 1, 1, 512])\n",
      "shapes torch.Size([1, 16, 1, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m main(cfg)\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/mtm/research/mtm/train.py:680\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hydra_cfg)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(hydra_cfg):\n\u001b[0;32m--> 680\u001b[0m     _main(hydra_cfg)\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/mtm/research/mtm/train.py:1041\u001b[0m, in \u001b[0;36m_main\u001b[0;34m(hydra_cfg)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     masks[\u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m masks[\u001b[39m\"\u001b[39m\u001b[39mstates\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1040\u001b[0m batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(cfg\u001b[39m.\u001b[39mdevice, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1041\u001b[0m _log_dict \u001b[39m=\u001b[39m train_one_batch(\n\u001b[1;32m   1042\u001b[0m     model,\n\u001b[1;32m   1043\u001b[0m     optimizer,\n\u001b[1;32m   1044\u001b[0m     scheduler,\n\u001b[1;32m   1045\u001b[0m     tokenizer_manager,\n\u001b[1;32m   1046\u001b[0m     discrete_map,\n\u001b[1;32m   1047\u001b[0m     batch,\n\u001b[1;32m   1048\u001b[0m     masks,\n\u001b[1;32m   1049\u001b[0m )\n\u001b[1;32m   1050\u001b[0m log_dict\u001b[39m.\u001b[39mupdate(_log_dict)\n\u001b[1;32m   1051\u001b[0m \u001b[39m# log train step time = time to process a batch\u001b[39;00m\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/mtm/research/mtm/train.py:664\u001b[0m, in \u001b[0;36mtrain_one_batch\u001b[0;34m(model, optimizer, scheduler, tokenizer_manager, discrete_map, batch, masks, loss_keys)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39m# backprop\u001b[39;00m\n\u001b[1;32m    663\u001b[0m model\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 664\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    665\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    666\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def get_mtm_model(\n",
    "    path: str,\n",
    ") -> Tuple[MTM, TokenizerManager, Dict[str, Tuple[int, int]]]:\n",
    "    def _get_dataset(dataset, traj_length):\n",
    "        return hydra.utils.call(dataset, seq_steps=traj_length)\n",
    "\n",
    "    # find checkpoints in the directory\n",
    "    steps = []\n",
    "    names = []\n",
    "    paths_ = os.listdir(path)\n",
    "    for name in [os.path.join(path, n) for n in paths_ if \"pt\" in n]:\n",
    "        step = os.path.basename(name).split(\"_\")[-1].split(\".\")[0]\n",
    "        steps.append(int(step))\n",
    "        names.append(name)\n",
    "    ckpt_path = names[np.argmax(steps)]\n",
    "\n",
    "    hydra_cfg = OmegaConf.load(os.path.join(path, \"config.yaml\"))\n",
    "    cfg = hydra.utils.instantiate(hydra_cfg.args)\n",
    "    train_dataset, val_dataset = _get_dataset(hydra_cfg.dataset, cfg.traj_length)\n",
    "    tokenizers: Dict[str, Tokenizer] = {\n",
    "        k: hydra.utils.call(v, key=k, train_dataset=train_dataset)\n",
    "        for k, v in hydra_cfg.tokenizers.items()\n",
    "    }\n",
    "    tokenizer_manager = TokenizerManager(tokenizers)\n",
    "    discrete_map: Dict[str, bool] = {}\n",
    "    for k, v in tokenizers.items():\n",
    "        discrete_map[k] = v.discrete\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        # shuffle=True,\n",
    "        pin_memory=True,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.n_workers,\n",
    "    )\n",
    "    train_batch = next(iter(train_loader))\n",
    "    tokenized = tokenizer_manager.encode(train_batch)\n",
    "    data_shapes = {}\n",
    "    for k, v in tokenized.items():\n",
    "        data_shapes[k] = v.shape[-2:]\n",
    "\n",
    "    model_config = hydra.utils.instantiate(hydra_cfg.model_config)\n",
    "    model = MTM(data_shapes, cfg.traj_length, model_config)\n",
    "    model.load_state_dict(torch.load(ckpt_path)[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    # freeze the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model, tokenizer_manager, data_shapes, val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5177b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer_manager, data_shapes, val_dataset = get_mtm_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27913aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    sampler=val_sampler,\n",
    ")\n",
    "val_batch = next(iter(val_loader))\n",
    "\n",
    "# visualize the data\n",
    "L = val_batch[\"states\"].shape[1]\n",
    "for states in val_batch[\"states\"][:4]:\n",
    "    plt.plot(np.arange(L), states, \"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = {\n",
    "    k: v.to(\"cpu\", non_blocking=True) for k, v in val_batch.items()\n",
    "}\n",
    "device = val_batch[\"states\"].device\n",
    "seq_len = val_batch[\"states\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c613877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate masks\n",
    "obs_mask = np.ones(seq_len)\n",
    "obs_mask[seq_len // 2 + 2 :] = 0 # mask out future observations\n",
    "obs_use_mask_list = [obs_mask]\n",
    "\n",
    "masks_list = []\n",
    "for obs_mask in obs_use_mask_list:\n",
    "    masks_list.append({\"states\": torch.from_numpy(obs_mask).to(device)})\n",
    "\n",
    "prefixs = [\"prediction\"]\n",
    "logs = make_plots_with_masks(model, val_batch, tokenizer_manager, masks_list, prefixs, batch_idxs = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "logs[\"prediction_eval/batch=0|0_states\"].image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f295dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "logs[\"prediction_eval/batch=1|0_states\"].image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
