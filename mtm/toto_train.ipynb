{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4802f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b37faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from research.mtm.models.mtm_model import MTM, make_plots_with_masks\n",
    "from research.mtm.tokenizers.base import Tokenizer, TokenizerManager\n",
    "from research.mtm.train import main, create_eval_logs_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5ee168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosmos/VScode Projects/AIRIproject/toto/mtm\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dda920",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/cosmos/VScode Projects/AIRIproject/toto/mtm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05956960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosmos/VScode Projects/AIRIproject/toto/mtm\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"research/mtm\"):\n",
    "    print(os.getcwd())\n",
    "    cfg = compose(config_name=\"config.yaml\", overrides=[\"+exp_mtm=toto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0073764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosmos/VScode Projects/AIRIproject/toto/mtm/toto\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"toto\", exist_ok=True)\n",
    "os.chdir(\"toto\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c631c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:research.mtm.distributed_utils:Loading distributed job info from submitit...\n",
      "WARNING:research.mtm.distributed_utils:Unable to load from submitit JobEnvironment: No module named 'submitit'\n",
      "INFO:research.mtm.distributed_utils:Loading distributed job info from environment variables...\n",
      "WARNING:research.mtm.distributed_utils:Unable to load from environment variables: 'RANK'\n",
      "INFO:root:Working directory: /home/cosmos/VScode Projects/AIRIproject/toto/mtm/toto\n",
      "INFO:root:Train set size = 105\n",
      "INFO:root:Validation set size = 105\n",
      "INFO:root:Tokenizers: {'observations': ContinuousTokenizer(), 'actions': ContinuousTokenizer(), 'rewards': ContinuousTokenizer()}\n",
      "INFO:root:\u001b[33m[stopwatch: data loader] Starting!\u001b[0m\n",
      "INFO:root:\u001b[33m[stopwatch: data loader] Completed in \u001b[1m0.0010766983032226562 seconds\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig(seed=0,\n",
      "          batch_size=512,\n",
      "          n_workers=10,\n",
      "          log_every=100,\n",
      "          print_every=1000,\n",
      "          eval_every=20000,\n",
      "          save_every=10000,\n",
      "          device='cpu',\n",
      "          mask_ratios=[0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
      "          mask_patterns=['AUTO_MASK'],\n",
      "          warmup_steps=40000,\n",
      "          num_train_steps=140010,\n",
      "          learning_rate=0.0001,\n",
      "          weight_decay=0.005,\n",
      "          traj_length=50,\n",
      "          mode_weights=[0.2, 0.1, 0.7],\n",
      "          tsp_ratio=1)\n",
      "<built-in function getcwd>\n",
      "Using 9 number of successful trajs. Total trajs: 10\n",
      "Using 9 number of successful trajs. Total trajs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data shapes: {'observations': torch.Size([1, 7]), 'actions': torch.Size([1, 7]), 'rewards': torch.Size([1, 1])}\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskvayzer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fe287f40884fdc9652c5331739c99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667220106670963, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>logs/d06b81f22cd7ed690251b92f99e1eaae/wandb/run-20230728_155815-d06b81f22cd7ed690251b92f99e1eaae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skvayzer/toto_test/runs/d06b81f22cd7ed690251b92f99e1eaae' target=\"_blank\">d06b81f22cd7ed690251b92f99e1eaae</a></strong> to <a href='https://wandb.ai/skvayzer/toto_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skvayzer/toto_test' target=\"_blank\">https://wandb.ai/skvayzer/toto_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skvayzer/toto_test/runs/d06b81f22cd7ed690251b92f99e1eaae' target=\"_blank\">https://wandb.ai/skvayzer/toto_test/runs/d06b81f22cd7ed690251b92f99e1eaae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:starting from step=0\n",
      "INFO:root:Step: 0, Train Loss: 1348.9874267578125\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train model\n",
    "main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def get_mtm_model(\n",
    "    path: str,\n",
    ") -> Tuple[MTM, TokenizerManager, Dict[str, Tuple[int, int]]]:\n",
    "    def _get_dataset(dataset, traj_length):\n",
    "        return hydra.utils.call(dataset, seq_steps=traj_length)\n",
    "\n",
    "    # find checkpoints in the directory\n",
    "    steps = []\n",
    "    names = []\n",
    "    paths_ = os.listdir(path)\n",
    "    for name in [os.path.join(path, n) for n in paths_ if \"pt\" in n]:\n",
    "        step = os.path.basename(name).split(\"_\")[-1].split(\".\")[0]\n",
    "        steps.append(int(step))\n",
    "        names.append(name)\n",
    "    ckpt_path = names[np.argmax(steps)]\n",
    "\n",
    "    hydra_cfg = OmegaConf.load(os.path.join(path, \"config.yaml\"))\n",
    "    cfg = hydra.utils.instantiate(hydra_cfg.args)\n",
    "    train_dataset, val_dataset = _get_dataset(hydra_cfg.dataset, cfg.traj_length)\n",
    "    tokenizers: Dict[str, Tokenizer] = {\n",
    "        k: hydra.utils.call(v, key=k, train_dataset=train_dataset)\n",
    "        for k, v in hydra_cfg.tokenizers.items()\n",
    "    }\n",
    "    tokenizer_manager = TokenizerManager(tokenizers)\n",
    "    discrete_map: Dict[str, bool] = {}\n",
    "    for k, v in tokenizers.items():\n",
    "        discrete_map[k] = v.discrete\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        # shuffle=True,\n",
    "        pin_memory=True,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.n_workers,\n",
    "    )\n",
    "    train_batch = next(iter(train_loader))\n",
    "    tokenized = tokenizer_manager.encode(train_batch)\n",
    "    data_shapes = {}\n",
    "    for k, v in tokenized.items():\n",
    "        data_shapes[k] = v.shape[-2:]\n",
    "\n",
    "    model_config = hydra.utils.instantiate(hydra_cfg.model_config)\n",
    "    model = MTM(data_shapes, cfg.traj_length, model_config)\n",
    "    model.load_state_dict(torch.load(ckpt_path)[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    # freeze the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model, tokenizer_manager, data_shapes, val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5177b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer_manager, data_shapes, val_dataset \u001b[39m=\u001b[39m get_mtm_model(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mget_mtm_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     steps\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39m(step))\n\u001b[1;32m     15\u001b[0m     names\u001b[39m.\u001b[39mappend(name)\n\u001b[0;32m---> 16\u001b[0m ckpt_path \u001b[39m=\u001b[39m names[np\u001b[39m.\u001b[39;49margmax(steps)]\n\u001b[1;32m     18\u001b[0m hydra_cfg \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     19\u001b[0m cfg \u001b[39m=\u001b[39m hydra\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39minstantiate(hydra_cfg\u001b[39m.\u001b[39margs)\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/VScode Projects/AIRIproject/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "model, tokenizer_manager, data_shapes, val_dataset = get_mtm_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27913aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    sampler=val_sampler,\n",
    ")\n",
    "val_batch = next(iter(val_loader))\n",
    "\n",
    "# visualize the data\n",
    "L = val_batch[\"states\"].shape[1]\n",
    "for states in val_batch[\"states\"][:4]:\n",
    "    plt.plot(np.arange(L), states, \"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = {\n",
    "    k: v.to(\"cpu\", non_blocking=True) for k, v in val_batch.items()\n",
    "}\n",
    "device = val_batch[\"states\"].device\n",
    "seq_len = val_batch[\"states\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c613877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate masks\n",
    "obs_mask = np.ones(seq_len)\n",
    "obs_mask[seq_len // 2 + 2 :] = 0 # mask out future observations\n",
    "obs_use_mask_list = [obs_mask]\n",
    "\n",
    "masks_list = []\n",
    "for obs_mask in obs_use_mask_list:\n",
    "    masks_list.append({\"states\": torch.from_numpy(obs_mask).to(device)})\n",
    "\n",
    "prefixs = [\"prediction\"]\n",
    "logs = make_plots_with_masks(model, val_batch, tokenizer_manager, masks_list, prefixs, batch_idxs = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "logs[\"prediction_eval/batch=0|0_states\"].image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f295dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "logs[\"prediction_eval/batch=1|0_states\"].image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
